{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "filXJVRiMSSs",
        "outputId": "3a3ff0d8-4c90-4148-e435-c5426557b76b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from google.cloud import storage\n",
        "from contextlib import closing\n",
        "import hashlib\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "PROJECT_ID = 'irproject2026'\n",
        "BUCKET_NAME = 'irprojectbucket'\n",
        "\n",
        "# INVERTED INDEX\n",
        "def get_bucket(bucket_name):\n",
        "    return storage.Client(PROJECT_ID).bucket(bucket_name)\n",
        "\n",
        "def _open(path, mode, bucket=None):\n",
        "    if bucket is None:\n",
        "        return open(path, mode)\n",
        "    return bucket.blob(path).open(mode)\n",
        "\n",
        "BLOCK_SIZE = 1999998\n",
        "\n",
        "class MultiFileWriter:\n",
        "    def __init__(self, base_dir, name, bucket_name=None):\n",
        "        self._base_dir = Path(base_dir)\n",
        "        self._name = name\n",
        "        self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
        "        self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n",
        "                                'wb', self._bucket)\n",
        "                          for i in itertools.count())\n",
        "        self._f = next(self._file_gen)\n",
        "\n",
        "    def write(self, b):\n",
        "        locs = []\n",
        "        while len(b) > 0:\n",
        "            pos = self._f.tell()\n",
        "            remaining = BLOCK_SIZE - pos\n",
        "            if remaining == 0:\n",
        "                self._f.close()\n",
        "                self._f = next(self._file_gen)\n",
        "                pos, remaining = 0, BLOCK_SIZE\n",
        "            self._f.write(b[:remaining])\n",
        "            name = self._f.name if hasattr(self._f, 'name') else self._f._blob.name\n",
        "            locs.append((name, pos))\n",
        "            b = b[remaining:]\n",
        "        return locs\n",
        "\n",
        "    def close(self):\n",
        "        self._f.close()\n",
        "\n",
        "class MultiFileReader:\n",
        "    def __init__(self, base_dir, bucket_name=None):\n",
        "        self._base_dir = Path(base_dir)\n",
        "        self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
        "        self._open_files = {}\n",
        "\n",
        "    def read(self, locs, n_bytes):\n",
        "        b = []\n",
        "        for f_name, offset in locs:\n",
        "            f_name = str(self._base_dir / f_name)\n",
        "            if f_name not in self._open_files:\n",
        "                self._open_files[f_name] = _open(f_name, 'rb', self._bucket)\n",
        "            f = self._open_files[f_name]\n",
        "            f.seek(offset)\n",
        "            n_read = min(n_bytes, BLOCK_SIZE - offset)\n",
        "            b.append(f.read(n_read))\n",
        "            n_bytes -= n_read\n",
        "        return b''.join(b)\n",
        "\n",
        "    def close(self):\n",
        "        for f in self._open_files.values():\n",
        "            f.close()\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.close()\n",
        "        return False\n",
        "\n",
        "TUPLE_SIZE = 6\n",
        "TF_MASK = 2 ** 16 - 1\n",
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self, docs={}):\n",
        "        self.df = Counter()\n",
        "        self.term_total = Counter()\n",
        "        self._posting_list = defaultdict(list)\n",
        "        self.posting_locs = defaultdict(list)\n",
        "        for doc_id, tokens in docs.items():\n",
        "            self.add_doc(doc_id, tokens)\n",
        "\n",
        "    def add_doc(self, doc_id, tokens):\n",
        "        w2cnt = Counter(tokens)\n",
        "        self.term_total.update(w2cnt)\n",
        "        for w, cnt in w2cnt.items():\n",
        "            self.df[w] = self.df.get(w, 0) + 1\n",
        "            self._posting_list[w].append((doc_id, cnt))\n",
        "\n",
        "    def write_index(self, base_dir, name, bucket_name=None):\n",
        "        if not os.path.exists(base_dir):\n",
        "            os.makedirs(base_dir, exist_ok=True)\n",
        "        \n",
        "        # Write global index locally\n",
        "        path = str(Path(base_dir) / f'{name}.pkl')\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "        \n",
        "        # Upload if bucket provided\n",
        "        if bucket_name:\n",
        "            client = storage.Client(PROJECT_ID)\n",
        "            bucket = client.bucket(bucket_name)\n",
        "            blob_path = f\"{base_dir}/{name}.pkl\"\n",
        "            bucket.blob(blob_path).upload_from_filename(path)\n",
        "            # Cleanup\n",
        "            try:\n",
        "                os.remove(path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        del state['_posting_list']\n",
        "        return state\n",
        "\n",
        "    @staticmethod\n",
        "    def write_a_posting_list(b_w_pl, base_dir, bucket_name=None):\n",
        "        posting_locs = defaultdict(list)\n",
        "        bucket_id, list_w_pl = b_w_pl\n",
        "\n",
        "        # Ensure local dir exists\n",
        "        if not os.path.exists(base_dir):\n",
        "            os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "        with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n",
        "            for w, pl in list_w_pl:\n",
        "                b = b''.join([(doc_id << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n",
        "                              for doc_id, tf in pl])\n",
        "                locs = writer.write(b)\n",
        "                posting_locs[w].extend(locs)\n",
        "            \n",
        "            # Save posting_locs dict locally first\n",
        "            path = str(Path(base_dir) / f'{bucket_id}_posting_locs.pickle')\n",
        "            with open(path, 'wb') as f:\n",
        "                pickle.dump(posting_locs, f)\n",
        "            \n",
        "            # Upload if bucket\n",
        "            if bucket_name:\n",
        "                bucket = get_bucket(bucket_name)\n",
        "                blob_path = f\"{base_dir}/{bucket_id}_posting_locs.pickle\"\n",
        "                bucket.blob(blob_path).upload_from_filename(path)\n",
        "                try:\n",
        "                    os.remove(path)\n",
        "                except:\n",
        "                    pass\n",
        "                    \n",
        "        return bucket_id\n",
        "\n",
        "# HELPERS\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "NUM_BUCKETS = 124\n",
        "\n",
        "\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def word_count(text, id):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  filtered_tokens = [token for token in tokens if token not in all_stopwords]\n",
        "  word_counts = Counter(filtered_tokens)\n",
        "  result = [(word, (id, count)) for word, count in word_counts.items()]\n",
        "  return result\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "  return sorted(unsorted_pl, key=lambda x: x[0])\n",
        "\n",
        "def calculate_df(postings):\n",
        "  return postings.mapValues(len)\n",
        "\n",
        "\n",
        "def partition_postings_and_write(postings, bucket_name, folder_name):\n",
        "  postings = postings.filter(lambda x: len(x[1]) < 2000000)\n",
        "  bucketed_postings = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1])))\n",
        "  grouped_by_bucket = bucketed_postings.groupByKey()\n",
        "  posting_locs = grouped_by_bucket.map(\n",
        "      lambda x: InvertedIndex.write_a_posting_list((x[0], list(x[1])), folder_name, bucket_name)\n",
        "  )\n",
        "  return posting_locs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "UOVzTv3XPbZP",
        "outputId": "e1bae316-0a93-488b-a1f2-60f554327668"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2776158926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q pyspark'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output, project_id)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_check_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CredentialType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_auth_ephem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       _message.blocking_request(\n\u001b[0m\u001b[1;32m    261\u001b[0m           \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'auth_user_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from google.cloud import storage\n",
        "from contextlib import closing\n",
        "import hashlib\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "PROJECT_ID = 'irproject2026'\n",
        "BUCKET_NAME = 'irprojectbucket'\n",
        "\n",
        "# Paths\n",
        "WIKI_PATH = f\"gs://{BUCKET_NAME}/*.parquet\" \n",
        "PV_PATH = f\"gs://{BUCKET_NAME}/pageviews-202108-user.bz2\"\n",
        "\n",
        "# --- INIT SPARK ---\n",
        "spark = SparkSession.builder     .appName(\"IR_Project_Index\")     .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\")     .config(\"spark.driver.memory\", \"8g\")     .config(\"spark.executor.memory\", \"8g\")     .config(\"spark.master\", \"local[*]\")     .getOrCreate()\n",
        "\n",
        "# Load Data\n",
        "parquetFile = spark.read.parquet(WIKI_PATH)\n",
        "\n",
        "# --- UTILS & CLASSES ---\n",
        "\n",
        "def get_bucket(bucket_name):\n",
        "    return storage.Client(PROJECT_ID).bucket(bucket_name)\n",
        "\n",
        "BLOCK_SIZE = 1999998\n",
        "\n",
        "class MultiFileWriter:\n",
        "    def __init__(self, base_dir, name, bucket_name=None):\n",
        "        self._base_dir = Path(base_dir)\n",
        "        self._name = name\n",
        "        self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
        "        self._file_gen = (self._open_local_file(i) for i in itertools.count())\n",
        "        self._f, self._current_filename = next(self._file_gen)\n",
        "\n",
        "    def _open_local_file(self, i):\n",
        "        filename = f'{self._name}_{i:03}.bin'\n",
        "        path = str(self._base_dir / filename)\n",
        "        if not os.path.exists(self._base_dir):\n",
        "            os.makedirs(self._base_dir, exist_ok=True)\n",
        "        return open(path, 'wb'), filename\n",
        "\n",
        "    def _upload_and_close_current(self):\n",
        "        if self._f:\n",
        "            filepath = self._f.name\n",
        "            self._f.close()\n",
        "            if self._bucket:\n",
        "                blob_path = f\"{self._base_dir}/{self._current_filename}\"\n",
        "                blob = self._bucket.blob(blob_path)\n",
        "                blob.upload_from_filename(filepath)\n",
        "                try:\n",
        "                    os.remove(filepath)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    def write(self, b):\n",
        "        locs = []\n",
        "        while len(b) > 0:\n",
        "            pos = self._f.tell()\n",
        "            remaining = BLOCK_SIZE - pos\n",
        "            if remaining == 0:\n",
        "                self._upload_and_close_current()\n",
        "                self._f, self._current_filename = next(self._file_gen)\n",
        "                pos, remaining = 0, BLOCK_SIZE\n",
        "            self._f.write(b[:remaining])\n",
        "            locs.append((self._current_filename, pos))\n",
        "            b = b[remaining:]\n",
        "        return locs\n",
        "\n",
        "    def close(self):\n",
        "        self._upload_and_close_current()\n",
        "\n",
        "TUPLE_SIZE = 6\n",
        "TF_MASK = 2 ** 16 - 1\n",
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self, docs={}):\n",
        "        self.df = Counter()\n",
        "        self.term_total = Counter()\n",
        "        self._posting_list = defaultdict(list)\n",
        "        self.posting_locs = defaultdict(list)\n",
        "        for doc_id, tokens in docs.items():\n",
        "            self.add_doc(doc_id, tokens)\n",
        "\n",
        "    def add_doc(self, doc_id, tokens):\n",
        "        w2cnt = Counter(tokens)\n",
        "        self.term_total.update(w2cnt)\n",
        "        for w, cnt in w2cnt.items():\n",
        "            self.df[w] = self.df.get(w, 0) + 1\n",
        "            self._posting_list[w].append((doc_id, cnt))\n",
        "\n",
        "    def write_index(self, base_dir, name, bucket_name=None):\n",
        "        if not os.path.exists(base_dir):\n",
        "            os.makedirs(base_dir, exist_ok=True)\n",
        "            \n",
        "        # Write global index locally\n",
        "        path = str(Path(base_dir) / f'{name}.pkl')\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "        \n",
        "        # Upload if bucket provided\n",
        "        if bucket_name:\n",
        "            client = storage.Client(PROJECT_ID)\n",
        "            bucket = client.bucket(bucket_name)\n",
        "            blob_path = f\"{base_dir}/{name}.pkl\"\n",
        "            bucket.blob(blob_path).upload_from_filename(path)\n",
        "            try:\n",
        "                os.remove(path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        del state['_posting_list']\n",
        "        return state\n",
        "\n",
        "    @staticmethod\n",
        "    def write_a_posting_list(b_w_pl, base_dir, bucket_name=None):\n",
        "        posting_locs = defaultdict(list)\n",
        "        bucket_id, list_w_pl = b_w_pl\n",
        "\n",
        "        if not os.path.exists(base_dir):\n",
        "            os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "        with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n",
        "            for w, pl in list_w_pl:\n",
        "                b = b''.join([(doc_id << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n",
        "                              for doc_id, tf in pl])\n",
        "                locs = writer.write(b)\n",
        "                posting_locs[w].extend(locs)\n",
        "            \n",
        "            path = str(Path(base_dir) / f'{bucket_id}_posting_locs.pickle')\n",
        "            with open(path, 'wb') as f:\n",
        "                pickle.dump(posting_locs, f)\n",
        "            \n",
        "            if bucket_name:\n",
        "                bucket = get_bucket(bucket_name)\n",
        "                blob_path = f\"{base_dir}/{bucket_id}_posting_locs.pickle\"\n",
        "                bucket.blob(blob_path).upload_from_filename(path)\n",
        "                try:\n",
        "                    os.remove(path)\n",
        "                except:\n",
        "                    pass\n",
        "                    \n",
        "        return bucket_id\n",
        "\n",
        "# --- NLP HELPERS ---\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "NUM_BUCKETS = 124\n",
        "\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def word_count(text, id):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  filtered_tokens = [token for token in tokens if token not in all_stopwords]\n",
        "  word_counts = Counter(filtered_tokens)\n",
        "  result = [(word, (id, count)) for word, count in word_counts.items()]\n",
        "  return result\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "  return sorted(unsorted_pl, key=lambda x: x[0])\n",
        "\n",
        "def calculate_df(postings):\n",
        "  return postings.mapValues(len)\n",
        "\n",
        "def partition_postings_and_write(postings, bucket_name, folder_name):\n",
        "  postings = postings.filter(lambda x: len(x[1]) < 2000000)\n",
        "  bucketed_postings = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1])))\n",
        "  grouped_by_bucket = bucketed_postings.groupByKey()\n",
        "  posting_locs = grouped_by_bucket.map(\n",
        "      lambda x: InvertedIndex.write_a_posting_list((x[0], list(x[1])), folder_name, bucket_name)\n",
        "  )\n",
        "  return posting_locs\n",
        "\n",
        "# --- CREATE INDEX FUNCTION (FIXED) ---\n",
        "def create_index(source_col, index_folder_name, is_anchor=False):\n",
        "    print(f\"Starting index creation for: {source_col if source_col else 'Anchor'} -> {index_folder_name}\")\n",
        "    \n",
        "    if is_anchor:\n",
        "        rdd_pairs = parquetFile.select(\"id\", \"anchor_text\").rdd\n",
        "        anchor_pairs = rdd_pairs.flatMap(lambda row: [(link.id, link.text) for link in row.anchor_text])\n",
        "        grouped_anchors = anchor_pairs.groupByKey().mapValues(lambda texts: \" \".join(texts))\n",
        "        word_counts = grouped_anchors.flatMap(lambda x: word_count(x[1], x[0]))\n",
        "    else:\n",
        "        rdd_pairs = parquetFile.select(source_col, \"id\").rdd\n",
        "        word_counts = rdd_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "    \n",
        "    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "    \n",
        "    # Filter rare words to save memory\n",
        "    if source_col == \"text\":\n",
        "        postings = postings.filter(lambda x: len(x[1]) > 50)\n",
        "    \n",
        "    w2df = calculate_df(postings)\n",
        "    w2df_dict = w2df.collectAsMap()\n",
        "    \n",
        "    _ = partition_postings_and_write(postings, BUCKET_NAME, index_folder_name).collect()\n",
        "    \n",
        "    super_posting_locs = defaultdict(list)\n",
        "    client = storage.Client(PROJECT_ID)\n",
        "    bucket = client.bucket(BUCKET_NAME)\n",
        "    \n",
        "    for blob in bucket.list_blobs(prefix=index_folder_name):\n",
        "        if not blob.name.endswith(\"pickle\"):\n",
        "            continue\n",
        "        \n",
        "        local_temp = f\"temp_{blob.name.split('/')[-1]}\"\n",
        "        blob.download_to_filename(local_temp)\n",
        "        \n",
        "        with open(local_temp, \"rb\") as f:\n",
        "            posting_locs = pickle.load(f)\n",
        "            for k, v in posting_locs.items():\n",
        "                super_posting_locs[k].extend(v)\n",
        "        try:\n",
        "            os.remove(local_temp)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    inverted = InvertedIndex()\n",
        "    inverted.posting_locs = super_posting_locs\n",
        "    inverted.df = w2df_dict\n",
        "    \n",
        "    if not os.path.exists(index_folder_name):\n",
        "        os.makedirs(index_folder_name, exist_ok=True)\n",
        "\n",
        "    local_index_name = f'index_{index_folder_name.split(\"_\")[-1]}.pkl'\n",
        "    inverted.write_index(index_folder_name, local_index_name.replace('.pkl', ''), BUCKET_NAME)\n",
        "    \n",
        "    print(f\"Finished creating index for {index_folder_name}.\")\n",
        "\n",
        "# --- EXECUTION ---\n",
        "\n",
        "# 1. Body Index\n",
        "create_index(\"text\", \"postings_gcp_body\")\n",
        "\n",
        "# 2. Title Index\n",
        "create_index(\"title\", \"postings_gcp_title\")\n",
        "\n",
        "# 3. Anchor Text Index\n",
        "create_index(None, \"postings_gcp_anchor\", is_anchor=True)\n",
        "\n",
        "# --- PAGERANK ---\n",
        "print(\"Starting PageRank...\")\n",
        "from graphframes import *\n",
        "\n",
        "def generate_graph(pages):\n",
        "    edges = pages.flatMap(lambda row: [(row.id, link.id) for link in row.anchor_text]).distinct()\n",
        "    vertices = pages.flatMap(lambda row: [(row.id,) for link in row.anchor_text] + [(row.id,)]).distinct().map(lambda x: (x[0], x[0]))\n",
        "    return edges, vertices\n",
        "\n",
        "pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd\n",
        "edges_rdd, vertices_rdd = generate_graph(pages_links)\n",
        "\n",
        "edgesDF = edges_rdd.toDF([\"src\", \"dst\"]).repartition(124, \"src\")\n",
        "verticesDF = vertices_rdd.toDF([\"id\", \"title\"]).repartition(124, \"id\")\n",
        "\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=10)\n",
        "pr_rdd = pr_results.vertices.select(\"id\", \"pagerank\").rdd\n",
        "pr_dict = pr_rdd.collectAsMap()\n",
        "\n",
        "with open('pr.pkl', 'wb') as f:\n",
        "    pickle.dump(pr_dict, f)\n",
        "\n",
        "os.system(f\"gsutil cp pr.pkl gs://{BUCKET_NAME}/pr.pkl\")\n",
        "print(\"PageRank finished.\")\n",
        "\n",
        "# --- PAGEVIEWS ---\n",
        "print(\"Starting PageViews...\")\n",
        "pv_df = spark.read.text(PV_PATH)\n",
        "\n",
        "def parse_page_views(line):\n",
        "    parts = line.value.split(\" \")\n",
        "    try:\n",
        "        page_id = int(parts[0])\n",
        "        views = int(parts[2])\n",
        "        return (page_id, views)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "pv_rdd = pv_df.rdd.map(parse_page_views).filter(lambda x: x is not None)\n",
        "pv_dict = pv_rdd.collectAsMap()\n",
        "\n",
        "with open('pageviews.pkl', 'wb') as f:\n",
        "    pickle.dump(pv_dict, f)\n",
        "\n",
        "os.system(f\"gsutil cp pageviews.pkl gs://{BUCKET_NAME}/pageviews.pkl\")\n",
        "print(\"PageViews finished.\")\n",
        "\n",
        "# --- TITLE MAPPING ---\n",
        "print(\"Starting ID to Title mapping...\")\n",
        "def create_id_to_title_mapping():\n",
        "    df = parquetFile.select(\"id\", \"title\")\n",
        "    id_to_title = df.rdd.collectAsMap()\n",
        "\n",
        "    with open('id_to_title.pkl', 'wb') as f:\n",
        "        pickle.dump(id_to_title, f)\n",
        "\n",
        "    os.system(f\"gsutil cp id_to_title.pkl gs://{BUCKET_NAME}/id_to_title.pkl\")\n",
        "\n",
        "create_id_to_title_mapping()\n",
        "print(\"All tasks finished successfully!\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "filXJVRiMSSs",
        "outputId": "3a3ff0d8-4c90-4148-e435-c5426557b76b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from google.cloud import storage\n",
        "from contextlib import closing\n",
        "import hashlib\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "PROJECT_ID = 'irroject2026'\n",
        "BUCKET_NAME = 'ourbucketon'\n",
        "\n",
        "# INVERTED INDEX\n",
        "def get_bucket(bucket_name):\n",
        "    return storage.Client(PROJECT_ID).bucket(bucket_name)\n",
        "\n",
        "def _open(path, mode, bucket=None):\n",
        "    if bucket is None:\n",
        "        return open(path, mode)\n",
        "    return bucket.blob(path).open(mode)\n",
        "\n",
        "BLOCK_SIZE = 1999998\n",
        "\n",
        "class MultiFileWriter:\n",
        "    def __init__(self, base_dir, name, bucket_name=None):\n",
        "        self._base_dir = Path(base_dir)\n",
        "        self._name = name\n",
        "        self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
        "        self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n",
        "                                'wb', self._bucket)\n",
        "                          for i in itertools.count())\n",
        "        self._f = next(self._file_gen)\n",
        "\n",
        "    def write(self, b):\n",
        "        locs = []\n",
        "        while len(b) > 0:\n",
        "            pos = self._f.tell()\n",
        "            remaining = BLOCK_SIZE - pos\n",
        "            if remaining == 0:\n",
        "                self._f.close()\n",
        "                self._f = next(self._file_gen)\n",
        "                pos, remaining = 0, BLOCK_SIZE\n",
        "            self._f.write(b[:remaining])\n",
        "            name = self._f.name if hasattr(self._f, 'name') else self._f._blob.name\n",
        "            locs.append((name, pos))\n",
        "            b = b[remaining:]\n",
        "        return locs\n",
        "\n",
        "    def close(self):\n",
        "        self._f.close()\n",
        "\n",
        "class MultiFileReader:\n",
        "    def __init__(self, base_dir, bucket_name=None):\n",
        "        self._base_dir = Path(base_dir)\n",
        "        self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
        "        self._open_files = {}\n",
        "\n",
        "    def read(self, locs, n_bytes):\n",
        "        b = []\n",
        "        for f_name, offset in locs:\n",
        "            f_name = str(self._base_dir / f_name)\n",
        "            if f_name not in self._open_files:\n",
        "                self._open_files[f_name] = _open(f_name, 'rb', self._bucket)\n",
        "            f = self._open_files[f_name]\n",
        "            f.seek(offset)\n",
        "            n_read = min(n_bytes, BLOCK_SIZE - offset)\n",
        "            b.append(f.read(n_read))\n",
        "            n_bytes -= n_read\n",
        "        return b''.join(b)\n",
        "\n",
        "    def close(self):\n",
        "        for f in self._open_files.values():\n",
        "            f.close()\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.close()\n",
        "        return False\n",
        "\n",
        "TUPLE_SIZE = 6\n",
        "TF_MASK = 2 ** 16 - 1\n",
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self, docs={}):\n",
        "        self.df = Counter()\n",
        "        self.term_total = Counter()\n",
        "        self._posting_list = defaultdict(list)\n",
        "        self.posting_locs = defaultdict(list)\n",
        "        for doc_id, tokens in docs.items():\n",
        "            self.add_doc(doc_id, tokens)\n",
        "\n",
        "    def add_doc(self, doc_id, tokens):\n",
        "        w2cnt = Counter(tokens)\n",
        "        self.term_total.update(w2cnt)\n",
        "        for w, cnt in w2cnt.items():\n",
        "            self.df[w] = self.df.get(w, 0) + 1\n",
        "            self._posting_list[w].append((doc_id, cnt))\n",
        "\n",
        "    def write_index(self, base_dir, name, bucket_name=None):\n",
        "        self._write_globals(base_dir, name, bucket_name)\n",
        "\n",
        "    def _write_globals(self, base_dir, name, bucket_name):\n",
        "        path = str(Path(base_dir) / f'{name}.pkl')\n",
        "        bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
        "        with _open(path, 'wb', bucket) as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        del state['_posting_list']\n",
        "        return state\n",
        "\n",
        "    @staticmethod\n",
        "    def write_a_posting_list(b_w_pl, base_dir, bucket_name=None):\n",
        "        posting_locs = defaultdict(list)\n",
        "        bucket_id, list_w_pl = b_w_pl\n",
        "\n",
        "        with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n",
        "            for w, pl in list_w_pl:\n",
        "                b = b''.join([(doc_id << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n",
        "                              for doc_id, tf in pl])\n",
        "                locs = writer.write(b)\n",
        "                posting_locs[w].extend(locs)\n",
        "            path = str(Path(base_dir) / f'{bucket_id}_posting_locs.pickle')\n",
        "            bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
        "            with _open(path, 'wb', bucket) as f:\n",
        "                pickle.dump(posting_locs, f)\n",
        "        return bucket_id\n",
        "\n",
        "# HELPERS\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "NUM_BUCKETS = 124\n",
        "\n",
        "\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def word_count(text, id):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  filtered_tokens = [token for token in tokens if token not in all_stopwords]\n",
        "  word_counts = Counter(filtered_tokens)\n",
        "  result = [(word, (id, count)) for word, count in word_counts.items()]\n",
        "  return result\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "  return sorted(unsorted_pl, key=lambda x: x[0])\n",
        "\n",
        "def calculate_df(postings):\n",
        "  return postings.mapValues(len)\n",
        "\n",
        "\n",
        "def partition_postings_and_write(postings, bucket_name, folder_name):\n",
        "  postings = postings.filter(lambda x: len(x[1]) < 2000000)\n",
        "  bucketed_postings = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1])))\n",
        "  grouped_by_bucket = bucketed_postings.groupByKey()\n",
        "  posting_locs = grouped_by_bucket.map(\n",
        "      lambda x: InvertedIndex.write_a_posting_list((x[0], list(x[1])), folder_name, bucket_name)\n",
        "  )\n",
        "  return posting_locs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "UOVzTv3XPbZP",
        "outputId": "e1bae316-0a93-488b-a1f2-60f554327668"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2776158926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q pyspark'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output, project_id)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_check_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CredentialType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_auth_ephem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       _message.blocking_request(\n\u001b[0m\u001b[1;32m    261\u001b[0m           \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'auth_user_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"IR_Project_Index\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "full_path = \"gs://wikidata_preprocessed/*\"\n",
        "parquetFile = spark.read.parquet(full_path)\n",
        "\n",
        "def create_index(source_col, index_folder_name):\n",
        "    print(f\"Starting index creation for: {source_col} -> {index_folder_name}\")\n",
        "    rdd_pairs = parquetFile.select(source_col, \"id\").rdd\n",
        "    word_counts = rdd_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "    w2df = calculate_df(postings)\n",
        "    w2df_dict = w2df.collectAsMap()\n",
        "    _ = partition_postings_and_write(postings, BUCKET_NAME, index_folder_name).collect()\n",
        "    super_posting_locs = defaultdict(list)\n",
        "    client = storage.Client(PROJECT_ID)\n",
        "    bucket = client.bucket(BUCKET_NAME)\n",
        "    for blob in bucket.list_blobs(prefix=index_folder_name):\n",
        "        if not blob.name.endswith(\"pickle\"):\n",
        "            continue\n",
        "        with blob.open(\"rb\") as f:\n",
        "            posting_locs = pickle.load(f)\n",
        "            for k, v in posting_locs.items():\n",
        "                super_posting_locs[k].extend(v)\n",
        "\n",
        "    inverted = InvertedIndex()\n",
        "    inverted.posting_locs = super_posting_locs\n",
        "    inverted.df = w2df_dict\n",
        "    inverted.write_index('.', 'index')\n",
        "    index_src = \"index.pkl\"\n",
        "    index_dst = f'gs://{BUCKET_NAME}/{index_folder_name}/{index_src}'\n",
        "    os.system(f\"gsutil cp {index_src} {index_dst}\")\n",
        "\n",
        "    print(f\"Finished creating index for {source_col}.\")\n",
        "\n",
        "# Body Index\n",
        "create_index(\"text\", \"postings_gcp_body\")\n",
        "\n",
        "# Title Index\n",
        "create_index(\"title\", \"postings_gcp_title\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2tq0YbMTp7M"
      },
      "outputs": [],
      "source": [
        "# --- BLOCK FOR ANCHOR INDEX ONLY ---\n",
        "# להריץ רק אם צריך שיפור בציון!\n",
        "\"\"\"\n",
        "pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd\n",
        "\n",
        "def doc_to_anchor_text(row):\n",
        "    full_text = \" \".join([x.text for x in row.anchor_text])\n",
        "    return (full_text, row.id)\n",
        "\n",
        "rdd_anchor = pages_links.map(doc_to_anchor_text)\n",
        "\n",
        "word_counts = rdd_anchor.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "\n",
        "w2df = calculate_df(postings)\n",
        "w2df_dict = w2df.collectAsMap()\n",
        "\n",
        "index_folder_name = \"postings_gcp_anchor\"\n",
        "_ = partition_postings_and_write(postings, BUCKET_NAME, index_folder_name).collect()\n",
        "\n",
        "super_posting_locs = defaultdict(list)\n",
        "client = storage.Client(PROJECT_ID)\n",
        "bucket = client.bucket(BUCKET_NAME)\n",
        "\n",
        "for blob in bucket.list_blobs(prefix=index_folder_name):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "        continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "        posting_locs = pickle.load(f)\n",
        "        for k, v in posting_locs.items():\n",
        "            super_posting_locs[k].extend(v)\n",
        "\n",
        "inverted = InvertedIndex()\n",
        "inverted.posting_locs = super_posting_locs\n",
        "inverted.df = w2df_dict\n",
        "inverted.write_index('.', 'index')\n",
        "\n",
        "index_src = \"index.pkl\"\n",
        "index_dst = f'gs://{BUCKET_NAME}/{index_folder_name}/{index_src}'\n",
        "os.system(f\"gsutil cp {index_src} {index_dst}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fccx2-yRUqwM"
      },
      "outputs": [],
      "source": [
        "# PAGERANK\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "!pip install graphframes\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PageRank\") \\\n",
        "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "from graphframes import *\n",
        "\n",
        "BUCKET_NAME = 'YOUR_BUCKET_NAME_HERE'\n",
        "full_path = \"gs://wikidata_precomputed/wiki_dump_parquet_schema\"\n",
        "parquetFile = spark.read.parquet(full_path)\n",
        "\n",
        "def generate_graph(pages):\n",
        "    edges = pages.flatMap(lambda row: [(row.id, link.id) for link in row.anchor_text]).distinct()\n",
        "    vertices = pages.flatMap(lambda row: [(row.id,) for link in row.anchor_text] + [(row.id,)]).distinct().map(lambda x: (x[0], x[0]))\n",
        "    return edges, vertices\n",
        "\n",
        "pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd\n",
        "edges_rdd, vertices_rdd = generate_graph(pages_links)\n",
        "\n",
        "edgesDF = edges_rdd.toDF([\"src\", \"dst\"]).repartition(124, \"src\")\n",
        "verticesDF = vertices_rdd.toDF([\"id\", \"title\"]).repartition(124, \"id\")\n",
        "\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=10)\n",
        "\n",
        "pr_rdd = pr_results.vertices.select(\"id\", \"pagerank\").rdd\n",
        "pr_dict = pr_rdd.collectAsMap()\n",
        "\n",
        "with open('pr.pkl', 'wb') as f:\n",
        "    pickle.dump(pr_dict, f)\n",
        "\n",
        "index_dst = f'gs://{BUCKET_NAME}/pr.pkl'\n",
        "os.system(f\"gsutil cp pr.pkl {index_dst}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6EjlCW0Ymrp"
      },
      "outputs": [],
      "source": [
        "# PAGE VIEWS\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "pv_path = 'gs://wikidata_precomputed/pageviews/2021/2021-08/pageviews-202108-user.bz2'\n",
        "BUCKET_NAME = 'ourbucketon'\n",
        "\n",
        "pv_df = spark.read.text(pv_path)\n",
        "\n",
        "def parse_page_views(line):\n",
        "    parts = line.value.split(\" \")\n",
        "    try:\n",
        "        page_id = int(parts[0])\n",
        "        views = int(parts[2])\n",
        "        return (page_id, views)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "pv_rdd = pv_df.rdd.map(parse_page_views).filter(lambda x: x is not None)\n",
        "pv_dict = pv_rdd.collectAsMap()\n",
        "\n",
        "with open('pageviews.pkl', 'wb') as f:\n",
        "    pickle.dump(pv_dict, f)\n",
        "\n",
        "dest_path = f'gs://{BUCKET_NAME}/pageviews.pkl'\n",
        "os.system(f\"gsutil cp pageviews.pkl {dest_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JRVhfVB5h6U"
      },
      "outputs": [],
      "source": [
        "def create_id_to_title_mapping():\n",
        "    full_path = \"gs://wikidata_precomputed/wiki_dump_parquet_schema\"\n",
        "    df = spark.read.parquet(full_path).select(\"id\", \"title\")\n",
        "\n",
        "    id_to_title = df.rdd.collectAsMap()\n",
        "\n",
        "    with open('id_to_title.pkl', 'wb') as f:\n",
        "        pickle.dump(id_to_title, f)\n",
        "\n",
        "    client = storage.Client(PROJECT_ID)\n",
        "    bucket = client.bucket(BUCKET_NAME)\n",
        "    blob = bucket.blob('id_to_title.pkl')\n",
        "    blob.upload_from_filename('id_to_title.pkl')\n",
        "\n",
        "create_id_to_title_mapping()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

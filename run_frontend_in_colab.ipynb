{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCPDHP7zTQJZ",
        "outputId": "c75f4593-706c-4d2d-ddca-b3261fb845e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download nltk stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INVERTD INDEX\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from contextlib import closing\n",
        "\n",
        "TUPLE_SIZE = 6\n",
        "TF_MASK = 2 ** 16 - 1\n",
        "\n",
        "class MultiFileReader:\n",
        "    def __init__(self, base_dir, bucket_name=None):\n",
        "        self._base_dir = Path(base_dir)\n",
        "        self._open_files = {}\n",
        "\n",
        "    def read(self, locs, n_bytes):\n",
        "        b = []\n",
        "        for f_name, offset in locs:\n",
        "            f_name = str(self._base_dir / f_name)\n",
        "            if f_name not in self._open_files:\n",
        "                self._open_files[f_name] = open(f_name, 'rb')\n",
        "            f = self._open_files[f_name]\n",
        "            f.seek(offset)\n",
        "            n_read = min(n_bytes, 1999998 - offset) # BLOCK_SIZE approximation\n",
        "            b.append(f.read(n_read))\n",
        "            n_bytes -= n_read\n",
        "        return b''.join(b)\n",
        "\n",
        "    def close(self):\n",
        "        for f in self._open_files.values():\n",
        "            f.close()\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.close()\n",
        "        return False\n",
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self, docs={}):\n",
        "        self.df = Counter()\n",
        "        self.term_total = Counter()\n",
        "        self.posting_locs = defaultdict(list)\n",
        "        for doc_id, tokens in docs.items():\n",
        "            self.add_doc(doc_id, tokens)\n",
        "\n",
        "    @staticmethod\n",
        "    def read_index(base_dir, name):\n",
        "        with open(Path(base_dir) / f'{name}.pkl', 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    def read_a_posting_list(self, base_dir, w, bucket_name=None):\n",
        "        posting_list = []\n",
        "        if not w in self.posting_locs:\n",
        "            return posting_list\n",
        "        with closing(MultiFileReader(base_dir, bucket_name)) as reader:\n",
        "            locs = self.posting_locs[w]\n",
        "            b = reader.read(locs, self.df[w] * TUPLE_SIZE)\n",
        "            for i in range(self.df[w]):\n",
        "                doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
        "                tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
        "                posting_list.append((doc_id, tf))\n",
        "        return posting_list"
      ],
      "metadata": {
        "id": "lqgxIWl5a2q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BM25\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "class BM25_from_index:\n",
        "    def __init__(self, index, k1=1.5, b=0.75):\n",
        "        self.index = index\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.N = len(index.posting_locs)\n",
        "        self.avgdl = 300\n",
        "\n",
        "    def calc_idf(self, list_of_tokens):\n",
        "        idf = {}\n",
        "        for term in list_of_tokens:\n",
        "            if term in self.index.df:\n",
        "                n_ti = self.index.df[term]\n",
        "                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))\n",
        "            else:\n",
        "                pass\n",
        "        return idf\n",
        "\n",
        "    def search(self, query, posting_lists_cache=None):\n",
        "        idf = self.calc_idf(query)\n",
        "        scores = Counter()\n",
        "\n",
        "        for term in query:\n",
        "            if term not in self.index.df:\n",
        "                continue\n",
        "            if posting_lists_cache and term in posting_lists_cache:\n",
        "                pls = posting_lists_cache[term]\n",
        "            else:\n",
        "                # הנחה: האינדקס יושב בתיקייה בשם postings_gcp_body\n",
        "                # לשנות את הנתיב לפי איפה שהקבצים שמורים ב-Colab\n",
        "                pls = self.index.read_a_posting_list(\"postings_gcp_body\", term)\n",
        "\n",
        "            current_idf = idf.get(term, 0)\n",
        "\n",
        "            for doc_id, tf in pls:\n",
        "                numerator = current_idf * tf * (self.k1 + 1)\n",
        "                denominator = tf + self.k1 * (1 - self.b + self.b * (1)) # dl/avgdl = 1 assumption\n",
        "                score = numerator / denominator\n",
        "\n",
        "                scores[doc_id] += score\n",
        "\n",
        "        return scores"
      ],
      "metadata": {
        "id": "ucmlIYU2a7zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAt6KT8xOgHH",
        "outputId": "0c67a9ce-b162-49b9-b614-8c6d895b9fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m92.2/106.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.29.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.43.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install a particular version of `google-cloud-storage` because (oddly enough)\n",
        "# the  version on Colab and GCP is old. A dependency error below is okay.\n",
        "!pip install -q google-cloud-storage==1.43.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oKFly5jFLFn"
      },
      "outputs": [],
      "source": [
        "# authenticate below for Google Storage access as needed\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dW0y91OVu5J"
      },
      "source": [
        "# Run the app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7opNkV6uRHIv"
      },
      "outputs": [],
      "source": [
        "# you need to upload your implementation of search_app.py\n",
        "import search_frontend as se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTGXXYEXV5l8"
      },
      "outputs": [],
      "source": [
        "# uncomment the code below and execute to reload the module when you make\n",
        "# changes to search_frontend.py (after you upload again).\n",
        "# import importlib\n",
        "# importlib.reload(se)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7K7oFlfVhm5"
      },
      "outputs": [],
      "source": [
        "# find Colab's public URL\n",
        "from google.colab.output import eval_js\n",
        "server_url = eval_js(\"google.colab.kernel.proxyPort(5000)\")\n",
        "print(f\"\"\"Test your search engine by navigating to\n",
        "{server_url}search?query=hello+world\n",
        "This URL is only accessible from the same browser session. In other words, this\n",
        "will not be accessible from a different machine, browser, or incognito session.\n",
        "\"\"\")\n",
        "\n",
        "# Uncomment the following line of code to run the frontend in the main\n",
        "# process and wait for HTTP requests (colab will hang). The debug parameter\n",
        "# lets you see incoming requests and get debug print outs if exceptions occur.\n",
        "# se.run(debug=False, use_reloader=False)\n",
        "\n",
        "# Alternatively, the next few lines run the frontend in a background process.\n",
        "# Just don't forget to terminate the process when you update your search engine\n",
        "# or want to reload it.\n",
        "import multiprocessin, time\n",
        "proc = multiprocessing.Process(target=se.run,\n",
        "                               kwargs={\"debug\": True, \"use_reloader\": False,\n",
        "                                       \"host\": \"0.0.0.0\", \"port\": 5000})\n",
        "proc.start()\n",
        "\n",
        "time.sleep(1) # give Flask time to boot\n",
        "\n",
        "from google.colab.output import eval_js\n",
        "server_url = eval_js(\"google.colab.kernel.proxyPort(5000)\")\n",
        "\n",
        "print(f\"Open this URL:\\n{server_url}/search?query=hello+world\")\n",
        "# Use proc.terminate() to stop the process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na0MC_1nzDbi"
      },
      "source": [
        "# Testing your app\n",
        "\n",
        "Once your app is running you can query it. You can simply do that by clicking on the URL printed above (the one looking like https://XXXXX-5000-colab.googleusercontent.com/search?query=hello+world or by issuing an HTTP request through code (from colab).\n",
        "\n",
        "The code below shows how to issue a query from python. This is also how our testing code will issue queries to your search engine, so make sure to test your search engine this way after you deploy it to GCP and before submission. Command line instructions for deploying your search engine to GCP are available at `run_frontend_in_gcp.sh`. Note that we will not only issue training queries to your search engine, but also test queries, i.e. queries that you've never seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM5ePrRHojbG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('queries_train.json', 'rt') as f:\n",
        "  queries = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWimZWCOy3Ei"
      },
      "outputs": [],
      "source": [
        "def average_precision(true_list, predicted_list, k=40):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    precisions = []\n",
        "    for i,doc_id in enumerate(predicted_list):\n",
        "        if doc_id in true_set:\n",
        "            prec = (len(precisions)+1) / (i+1)\n",
        "            precisions.append(prec)\n",
        "    if len(precisions) == 0:\n",
        "        return 0.0\n",
        "    return round(sum(precisions)/len(precisions),3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geHKyFB4xkBe"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(true_list, predicted_list, k):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    if len(predicted_list) == 0:\n",
        "        return 0.0\n",
        "    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(predicted_list), 3)\n",
        "def recall_at_k(true_list, predicted_list, k):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    if len(true_set) < 1:\n",
        "        return 1.0\n",
        "    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(true_set), 3)\n",
        "def f1_at_k(true_list, predicted_list, k):\n",
        "    p = precision_at_k(true_list, predicted_list, k)\n",
        "    r = recall_at_k(true_list, predicted_list, k)\n",
        "    if p == 0.0 or r == 0.0:\n",
        "        return 0.0\n",
        "    return round(2.0 / (1.0/p + 1.0/r), 3)\n",
        "def results_quality(true_list, predicted_list):\n",
        "    p5 = precision_at_k(true_list, predicted_list, 5)\n",
        "    f1_30 = f1_at_k(true_list, predicted_list, 30)\n",
        "    if p5 == 0.0 or f1_30 == 0.0:\n",
        "        return 0.0\n",
        "    return round(2.0 / (1.0/p5 + 1.0/f1_30), 3)\n",
        "\n",
        "assert precision_at_k(range(10), [1,2,3] , 2) == 1.0\n",
        "assert recall_at_k(   range(10), [10,5,3], 2) == 0.1\n",
        "assert precision_at_k(range(10), []      , 2) == 0.0\n",
        "assert precision_at_k([],        [1,2,3],  5) == 0.0\n",
        "assert recall_at_k(   [],        [10,5,3], 2) == 1.0\n",
        "assert recall_at_k(   range(10), [],       2) == 0.0\n",
        "assert f1_at_k(       [],        [1,2,3],  5) == 0.0\n",
        "assert f1_at_k(       range(10), [],       2) == 0.0\n",
        "assert f1_at_k(       range(10), [0,1,2],  2) == 0.333\n",
        "assert f1_at_k(       range(50), range(5), 30) == 0.182\n",
        "assert f1_at_k(       range(50), range(10), 30) == 0.333\n",
        "assert f1_at_k(       range(50), range(30), 30) == 0.75\n",
        "assert results_quality(range(50), range(5))  == 0.308\n",
        "assert results_quality(range(50), range(10)) == 0.5\n",
        "assert results_quality(range(50), range(30)) == 0.857\n",
        "assert results_quality(range(50), [-1]*5 + list(range(5,30))) == 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYmNTq9u0ChK"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from time import time\n",
        "# In GCP the public URL for your engine should look like this:\n",
        "# url = 'http://35.232.59.3:8080'\n",
        "# In colab, we are going to send HTTP requests to localhost (127.0.0.1)\n",
        "# and direct them to port where the server is listening (5000).\n",
        "url = 'http://127.0.0.1:5000'\n",
        "\n",
        "qs_res = []\n",
        "for q, true_wids in queries.items():\n",
        "  duration, ap = None, None\n",
        "  t_start = time()\n",
        "  try:\n",
        "    res = requests.get(url + '/search', {'query': q}, timeout=35)\n",
        "    duration = time() - t_start\n",
        "    if res.status_code == 200:\n",
        "      pred_wids, _ = zip(*res.json())\n",
        "      rq = results_quality(true_wids, pred_wids)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  qs_res.append((q, duration, rq))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}